name: Sample Evaluate Action
on:
  workflow_call:
  workflow_dispatch:

permissions:
  id-token: write
  contents: read

jobs:
  evaluate:
    runs-on: ubuntu-latest
    env:
      # ✅ Point this to your existing JSONL
      GENAI_EVALS_DATA_PATH: ${{ github.workspace }}/.github/.test_files/eval-input.jsonl
      GENAI_EVALS_CONFIG_PATH: ${{ github.workspace }}/evaluate-config.json
    steps:
      - uses: actions/checkout@v4

      - uses: azure/login@v2
        with:
          client-id: ${{ secrets.OIDC_AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.OIDC_AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.OIDC_AZURE_SUBSCRIPTION_ID }}

      # ✅ Validate your dataset lines so Coherence/Fluency won't error
      - name: Validate dataset schema (conversation OR input+response)
        run: |
          python - <<'PY'
          import json, pathlib, sys, io
          p = pathlib.Path("${{ env.GENAI_EVALS_DATA_PATH }}")
          if not p.exists():
            print("Dataset not found:", p, file=sys.stderr); sys.exit(1)

          bad = []
          with p.open("r", encoding="utf-8") as f:
            for i, line in enumerate(f, 1):
              line = line.strip()
              if not line: 
                bad.append((i, "empty line")); continue
              try:
                row = json.loads(line)
              except json.JSONDecodeError as e:
                bad.append((i, f"invalid JSON: {e}")); continue

              ok = False
              # Accept either conversation[] or input+response (or prompt+completion)
              if isinstance(row.get("conversation"), list) and row["conversation"]:
                ok = True
              elif all(k in row for k in ("input","response")):
                ok = True
              elif all(k in row for k in ("prompt","completion")):
                ok = True

              if not ok:
                bad.append((i, "missing conversation[] or (input+response) or (prompt+completion)"))

          if bad:
            print("Dataset has schema issues:", file=sys.stderr)
            for i, msg in bad[:20]:
              print(f"  line {i}: {msg}", file=sys.stderr)
            if len(bad) > 20:
              print(f"  ...and {len(bad)-20} more", file=sys.stderr)
            sys.exit(2)
          print("Dataset looks OK:", p)
          PY

      # ✅ Write evaluate-config.json without here-docs
      - name: Write evaluate config (Python)
        env:
          AZURE_OPENAI_ENDPOINT: ${{ secrets.AZURE_OPENAI_ENDPOINT }}
          AZURE_OPENAI_CHAT_DEPLOYMENT: ${{ secrets.AZURE_OPENAI_CHAT_DEPLOYMENT }}
          AZURE_OPENAI_API_KEY: ${{ secrets.AZURE_OPENAI_API_KEY }}
          AZURE_OPENAI_API_VERSION: ${{ secrets.AZURE_OPENAI_API_VERSION }}
        run: |
          python - <<'PY'
          import json, os, pathlib
          cfg = {
            "data": os.environ["GENAI_EVALS_DATA_PATH"],
            "evaluators": {
              "coherence": "CoherenceEvaluator",
              "fluency": "FluencyEvaluator"
            },
            "ai_model_configuration": {
              "type": "azure_openai",
              "azure_endpoint": os.environ["AZURE_OPENAI_ENDPOINT"],
              "azure_deployment": os.environ["AZURE_OPENAI_CHAT_DEPLOYMENT"],
              "api_key": os.environ["AZURE_OPENAI_API_KEY"],
              "api_version": os.environ["AZURE_OPENAI_API_VERSION"]
            }
          }
          p = pathlib.Path(os.environ["GENAI_EVALS_CONFIG_PATH"])
          p.write_text(json.dumps(cfg, indent=2), encoding="utf-8")
          print("Wrote", p, "OK")
          PY

      - name: Run AI Evaluation
        id: run-ai-evaluation
        uses: microsoft/genai-evals@main
        with:
          evaluate-configuration: ${{ env.GENAI_EVALS_CONFIG_PATH }}

      # Optional: surface results
      - name: Upload eval artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: genai-evals-artifacts
          path: |
            **/summary.md
            **/results.jsonl
          if-no-files-found: warn
